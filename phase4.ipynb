{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a25ac8-0665-4d34-a36b-26ad352b5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split meta: {'max_date': '2015-07-31', 'val_start': '2015-05-09', 'test_start': '2015-06-20', 'train_rows': 923549, 'val_rows': 46830, 'test_rows': 46830, 'test_days': 42, 'val_days': 42}\n",
      "Total rows: 1017209 | Num features: 59\n",
      "\n",
      "=== Horizon h=1 days ===\n",
      "Train/Val/Test: (837426, 59) (2043, 59) (1992, 59)\n",
      "\n",
      "=== Horizon h=2 days ===\n",
      "Train/Val/Test: (835299, 59) (1896, 59) (1848, 59)\n",
      "\n",
      "=== Horizon h=3 days ===\n",
      "Train/Val/Test: (833172, 59) (1753, 59) (1702, 59)\n",
      "\n",
      "=== Horizon h=4 days ===\n",
      "Train/Val/Test: (831045, 59) (1612, 59) (1566, 59)\n",
      "\n",
      "=== Horizon h=5 days ===\n",
      "Train/Val/Test: (828918, 59) (1480, 59) (1436, 59)\n",
      "\n",
      "=== Horizon h=6 days ===\n",
      "Train/Val/Test: (826791, 59) (1352, 59) (1310, 59)\n",
      "\n",
      "=== Horizon h=7 days ===\n",
      "Train/Val/Test: (824664, 59) (1234, 59) (1192, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_days</th>\n",
       "      <th>val_rmse_p50</th>\n",
       "      <th>val_rmspe_p50</th>\n",
       "      <th>val_mape_p50</th>\n",
       "      <th>val_r2_p50</th>\n",
       "      <th>val_pinball_p10</th>\n",
       "      <th>val_pinball_p50</th>\n",
       "      <th>val_pinball_p90</th>\n",
       "      <th>val_p10p90_coverage</th>\n",
       "      <th>val_p10p90_width</th>\n",
       "      <th>test_rmse_p50</th>\n",
       "      <th>test_rmspe_p50</th>\n",
       "      <th>test_mape_p50</th>\n",
       "      <th>test_r2_p50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6731.161133</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>-2.463251</td>\n",
       "      <td>567.685242</td>\n",
       "      <td>2838.398926</td>\n",
       "      <td>5109.112793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6992.097168</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-2.906716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6792.583008</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>-2.657975</td>\n",
       "      <td>579.024292</td>\n",
       "      <td>2895.084473</td>\n",
       "      <td>5211.145020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7079.646973</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-3.166268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6802.216309</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>-2.622007</td>\n",
       "      <td>578.760010</td>\n",
       "      <td>2893.769531</td>\n",
       "      <td>5208.778809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7126.923340</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-3.123093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6787.185059</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>-2.565333</td>\n",
       "      <td>575.729919</td>\n",
       "      <td>2878.608643</td>\n",
       "      <td>5181.486816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7137.979980</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-3.127309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6835.941406</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>-2.550572</td>\n",
       "      <td>579.393860</td>\n",
       "      <td>2896.936523</td>\n",
       "      <td>5214.479004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7157.231934</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-3.086039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6910.300293</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>-2.789315</td>\n",
       "      <td>592.884033</td>\n",
       "      <td>2964.395996</td>\n",
       "      <td>5335.907715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7186.795898</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-3.032157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6905.974121</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>-2.766487</td>\n",
       "      <td>591.871277</td>\n",
       "      <td>2959.319336</td>\n",
       "      <td>5326.767578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7163.399414</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-3.018601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   horizon_days  val_rmse_p50  val_rmspe_p50  val_mape_p50  val_r2_p50  \\\n",
       "0             1   6731.161133       0.999995      0.999995   -2.463251   \n",
       "1             2   6792.583008       0.999996      0.999996   -2.657975   \n",
       "2             3   6802.216309       0.999995      0.999995   -2.622007   \n",
       "3             4   6787.185059       0.999996      0.999996   -2.565333   \n",
       "4             5   6835.941406       0.999996      0.999996   -2.550572   \n",
       "5             6   6910.300293       0.999995      0.999995   -2.789315   \n",
       "6             7   6905.974121       0.999996      0.999996   -2.766487   \n",
       "\n",
       "   val_pinball_p10  val_pinball_p50  val_pinball_p90  val_p10p90_coverage  \\\n",
       "0       567.685242      2838.398926      5109.112793                  0.0   \n",
       "1       579.024292      2895.084473      5211.145020                  0.0   \n",
       "2       578.760010      2893.769531      5208.778809                  0.0   \n",
       "3       575.729919      2878.608643      5181.486816                  0.0   \n",
       "4       579.393860      2896.936523      5214.479004                  0.0   \n",
       "5       592.884033      2964.395996      5335.907715                  0.0   \n",
       "6       591.871277      2959.319336      5326.767578                  0.0   \n",
       "\n",
       "   val_p10p90_width  test_rmse_p50  test_rmspe_p50  test_mape_p50  test_r2_p50  \n",
       "0               0.0    6992.097168        0.999997       0.999997    -2.906716  \n",
       "1               0.0    7079.646973        0.999998       0.999998    -3.166268  \n",
       "2               0.0    7126.923340        0.999998       0.999998    -3.123093  \n",
       "3               0.0    7137.979980        0.999998       0.999998    -3.127309  \n",
       "4               0.0    7157.231934        0.999998       0.999998    -3.086039  \n",
       "5               0.0    7186.795898        0.999998       0.999998    -3.032157  \n",
       "6               0.0    7163.399414        0.999998       0.999998    -3.018601  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_days</th>\n",
       "      <th>K</th>\n",
       "      <th>val_rmse_mean</th>\n",
       "      <th>val_coverage_approx90</th>\n",
       "      <th>val_mean_std</th>\n",
       "      <th>val_mean_interval_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6905.995605</td>\n",
       "      <td>0.172609</td>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.240459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   horizon_days  K  val_rmse_mean  val_coverage_approx90  val_mean_std  \\\n",
       "0             7  7    6905.995605               0.172609      0.073092   \n",
       "\n",
       "   val_mean_interval_width  \n",
       "0                 0.240459  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification horizon h=1 ===\n",
      "Thresholds (Euros): Low < 4498.0 <= Med <= 7031.0 < High\n",
      "Train class counts: [279215 279105 279106]\n",
      "\n",
      "=== Classification horizon h=7 ===\n",
      "Thresholds (Euros): Low < 4500.0 <= Med <= 7033.0 < High\n",
      "Train class counts: [274957 274904 274803]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon_days</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>val_f1_macro</th>\n",
       "      <th>val_auc_ovr_macro</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_auc_ovr_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4498.000977</td>\n",
       "      <td>7031.002441</td>\n",
       "      <td>0.721119</td>\n",
       "      <td>0.884956</td>\n",
       "      <td>0.785705</td>\n",
       "      <td>0.931242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4500.000488</td>\n",
       "      <td>7033.000977</td>\n",
       "      <td>0.791585</td>\n",
       "      <td>0.922837</td>\n",
       "      <td>0.815225</td>\n",
       "      <td>0.944688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   horizon_days           t1           t2  val_f1_macro  val_auc_ovr_macro  \\\n",
       "0             1  4498.000977  7031.002441      0.721119           0.884956   \n",
       "1             7  4500.000488  7033.000977      0.791585           0.922837   \n",
       "\n",
       "   test_f1_macro  test_auc_ovr_macro  \n",
       "0       0.785705            0.931242  \n",
       "1       0.815225            0.944688  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 4 outputs saved in artifacts/phase4/\n",
      "Horizons trained: [1, 2, 3, 4, 5, 6, 7]\n",
      "Files:\n",
      "- metrics_multistep_quantile.csv\n",
      "- rmspe_vs_horizon.png\n",
      "- coverage_vs_horizon.png\n",
      "- interval_width_vs_horizon.png\n",
      "- forecast_sample_next_horizons.csv\n",
      "- metrics_classification.csv\n",
      "- metrics_ensemble_uncertainty.csv\n",
      "- ensemble_std_hist_h7.png\n",
      "- cm_val_h1.png / cm_test_h1.png\n",
      "- cm_val_h7.png / cm_test_h7.png\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Phase 4 — 7-day Multi-step + Uncertainty + Classification\n",
    "# + Report generator -> artifacts/phase4/\n",
    "# =========================================\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# A) Config\n",
    "# -------------------------\n",
    "PHASE2_DIR = \"artifacts/phase2\"\n",
    "OUT_DIR = \"artifacts/phase4\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_F = os.path.join(PHASE2_DIR, \"train_features.csv\")\n",
    "VAL_F   = os.path.join(PHASE2_DIR, \"val_features.csv\")\n",
    "TEST_F  = os.path.join(PHASE2_DIR, \"test_features.csv\")\n",
    "FEAT_F  = os.path.join(PHASE2_DIR, \"feature_cols.json\")\n",
    "META_F  = os.path.join(PHASE2_DIR, \"split_meta.json\")\n",
    "\n",
    "for p in [TRAIN_F, VAL_F, TEST_F, FEAT_F, META_F]:\n",
    "    assert os.path.exists(p), f\"Missing: {p}\"\n",
    "\n",
    "with open(FEAT_F, \"r\", encoding=\"utf-8\") as f:\n",
    "    feature_cols = json.load(f)[\"feature_cols\"]\n",
    "\n",
    "with open(META_F, \"r\", encoding=\"utf-8\") as f:\n",
    "    split_meta = json.load(f)\n",
    "\n",
    "if \"Store\" in feature_cols:\n",
    "    print(\"WARNING: 'Store' found in feature_cols.json — removing it.\")\n",
    "    feature_cols = [c for c in feature_cols if c != \"Store\"]\n",
    "\n",
    "TARGET = \"Sales\"\n",
    "META_COLS = [\"Store\", \"Date\"]\n",
    "\n",
    "\n",
    "H_FULL = 7\n",
    "\n",
    "HORIZONS = list(range(1, H_FULL + 1)) \n",
    "\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "Q_LR = 0.05\n",
    "Q_EPOCHS = 18\n",
    "Q_BATCH = 8192\n",
    "Q_L2 = 1e-4\n",
    "\n",
    "DO_ENSEMBLE = True\n",
    "ENSEMBLE_HORIZON = 7    \n",
    "ENSEMBLE_K = 7\n",
    "\n",
    "CLASS_HORIZONS = [1, 7] \n",
    "CLF_LR = 0.2\n",
    "CLF_EPOCHS = 22\n",
    "CLF_BATCH = 8192\n",
    "CLF_L2 = 1e-4\n",
    "\n",
    "SAMPLE_STORES_N = 10\n",
    "FORECAST_ORIGIN_MODE = \"before_test\" \n",
    "CUSTOM_ORIGIN_DATE = None \n",
    "\n",
    "# -------------------------\n",
    "# B) Helpers: IO + plots\n",
    "# -------------------------\n",
    "def save_json(path, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def save_df_csv(path, df):\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def save_fig(path):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "save_json(os.path.join(OUT_DIR, \"config_phase4.json\"), {\n",
    "    \"H_FULL\": H_FULL, \"HORIZONS\": HORIZONS,\n",
    "    \"QUANTILES\": QUANTILES, \"Q_LR\": Q_LR, \"Q_EPOCHS\": Q_EPOCHS,\n",
    "    \"Q_BATCH\": Q_BATCH, \"Q_L2\": Q_L2, \"DO_ENSEMBLE\": DO_ENSEMBLE,\n",
    "    \"ENSEMBLE_HORIZON\": ENSEMBLE_HORIZON, \"ENSEMBLE_K\": ENSEMBLE_K,\n",
    "    \"CLASS_HORIZONS\": CLASS_HORIZONS, \"CLF_LR\": CLF_LR,\n",
    "    \"CLF_EPOCHS\": CLF_EPOCHS, \"CLF_BATCH\": CLF_BATCH, \"CLF_L2\": CLF_L2,\n",
    "    \"SAMPLE_STORES_N\": SAMPLE_STORES_N, \"FORECAST_ORIGIN_MODE\": FORECAST_ORIGIN_MODE,\n",
    "    \"CUSTOM_ORIGIN_DATE\": CUSTOM_ORIGIN_DATE, \"split_meta\": split_meta,\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# C) Load Phase2 CSVs\n",
    "# -------------------------\n",
    "NROWS_DEBUG = None \n",
    "\n",
    "train_df = pd.read_csv(TRAIN_F, nrows=NROWS_DEBUG)\n",
    "val_df   = pd.read_csv(VAL_F,   nrows=NROWS_DEBUG)\n",
    "test_df  = pd.read_csv(TEST_F,  nrows=NROWS_DEBUG)\n",
    "\n",
    "def ensure_store_date(df):\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    s = pd.to_numeric(df[\"Store\"], errors=\"coerce\")\n",
    "    df[\"Store\"] = np.round(s).astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = ensure_store_date(train_df)\n",
    "val_df   = ensure_store_date(val_df)\n",
    "test_df  = ensure_store_date(test_df)\n",
    "\n",
    "df_all = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "df_all = df_all.sort_values([\"Store\",\"Date\"]).reset_index(drop=True)\n",
    "\n",
    "val_start  = pd.to_datetime(split_meta[\"val_start\"])\n",
    "test_start = pd.to_datetime(split_meta[\"test_start\"])\n",
    "max_date   = pd.to_datetime(split_meta[\"max_date\"])\n",
    "\n",
    "print(\"Split meta:\", split_meta)\n",
    "print(\"Total rows:\", len(df_all), \"| Num features:\", len(feature_cols))\n",
    "\n",
    "# -------------------------\n",
    "# D) Metrics (FIXED FOR LOG-TARGET)\n",
    "# -------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.expm1(y_true)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.expm1(y_true)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    mask = y_true > 0\n",
    "    if not np.any(mask): return 0.0\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    y_true = np.expm1(y_true)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    mask = y_true > 0\n",
    "    if not np.any(mask): return 0.0\n",
    "    return float(np.sqrt(np.mean(((y_true[mask] - y_pred[mask]) / y_true[mask]) ** 2)))\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    y_true = np.expm1(y_true)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2) + 1e-12\n",
    "    return float(1 - ss_res / ss_tot)\n",
    "\n",
    "def pinball_loss(y, yhat, q):\n",
    "    y = np.expm1(y)\n",
    "    yhat = np.expm1(yhat)\n",
    "    e = y - yhat\n",
    "    return float(np.mean(np.maximum(q*e, (q-1)*e)))\n",
    "\n",
    "# -------------------------\n",
    "# E) Build time-safe masks per horizon\n",
    "# -------------------------\n",
    "def split_masks_for_h(df, h):\n",
    "    train_end = val_start - pd.Timedelta(days=h)\n",
    "    val_end   = test_start - pd.Timedelta(days=h)\n",
    "    test_end  = max_date - pd.Timedelta(days=h)\n",
    "\n",
    "    m_train = (df[\"Date\"] <= train_end)\n",
    "    m_val   = (df[\"Date\"] >= val_start) & (df[\"Date\"] < test_start) & (df[\"Date\"] <= val_end)\n",
    "    m_test  = (df[\"Date\"] >= test_start) & (df[\"Date\"] <= test_end)\n",
    "    return m_train, m_val, m_test\n",
    "\n",
    "def build_xy_for_h(df, h, mask):\n",
    "    sub = df.loc[mask, :].copy()\n",
    "    sub = sub.sort_values([\"Store\",\"Date\"])\n",
    "    sub[\"y\"] = sub.groupby(\"Store\", sort=False)[TARGET].shift(-h)\n",
    "    sub = sub.dropna(subset=[\"y\"])\n",
    "    X = sub[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y = sub[\"y\"].to_numpy(dtype=np.float32)\n",
    "    meta = sub[META_COLS].copy()\n",
    "    return X, y, meta\n",
    "\n",
    "# -------------------------\n",
    "# F) Quantile Regression model (from scratch)\n",
    "# -------------------------\n",
    "class QuantileLinearGD:\n",
    "    def __init__(self, q=0.5, lr=0.05, epochs=20, batch_size=8192, l2=1e-4, seed=42, verbose=0):\n",
    "        self.q=q; self.lr=lr; self.epochs=epochs; self.batch_size=batch_size\n",
    "        self.l2=l2; self.seed=seed; self.verbose=verbose\n",
    "        self.w=None; self.b=0.0\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        n, d = X.shape\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.w = rng.normal(0, 0.01, size=(d,)).astype(np.float32)\n",
    "        self.b = 0.0\n",
    "\n",
    "        best = (float(\"inf\"), self.w.copy(), float(self.b))\n",
    "\n",
    "        for ep in range(1, self.epochs+1):\n",
    "            idx = rng.permutation(n)\n",
    "            Xs, ys = X[idx], y[idx]\n",
    "\n",
    "            for i in range(0, n, self.batch_size):\n",
    "                xb = Xs[i:i+self.batch_size]\n",
    "                yb = ys[i:i+self.batch_size]\n",
    "\n",
    "                pred = xb @ self.w + self.b\n",
    "                e = yb - pred\n",
    "\n",
    "                grad_pred = np.where(e >= 0, -self.q, -(self.q - 1.0)).astype(np.float32)\n",
    "\n",
    "                gw = (xb.T @ grad_pred) / len(xb)\n",
    "                gb = float(np.mean(grad_pred))\n",
    "\n",
    "                if self.l2 > 0:\n",
    "                    gw = gw + self.l2 * self.w\n",
    "\n",
    "                self.w -= self.lr * gw\n",
    "                self.b -= self.lr * gb\n",
    "\n",
    "            if X_val is not None and self.verbose:\n",
    "                pv = self.predict(X_val)\n",
    "                lv = pinball_loss(y_val, pv, self.q)\n",
    "                if lv < best[0]:\n",
    "                    best = (lv, self.w.copy(), float(self.b))\n",
    "                print(f\"Epoch {ep:02d} | Val pinball(q={self.q})={lv:.5f}\")\n",
    "\n",
    "        if X_val is not None:\n",
    "            _, self.w, self.b = best\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (X @ self.w + self.b).astype(np.float32)\n",
    "\n",
    "# -------------------------\n",
    "# G) Train multi-horizon quantile models + save metrics\n",
    "# -------------------------\n",
    "models_q = {}\n",
    "rows = []\n",
    "\n",
    "for h in HORIZONS:\n",
    "    m_tr, m_va, m_te = split_masks_for_h(df_all, h)\n",
    "    Xtr, ytr, _ = build_xy_for_h(df_all, h, m_tr)\n",
    "    Xva, yva, _ = build_xy_for_h(df_all, h, m_va)\n",
    "    Xte, yte, _ = build_xy_for_h(df_all, h, m_te)\n",
    "\n",
    "    print(f\"\\n=== Horizon h={h} days ===\")\n",
    "    print(\"Train/Val/Test:\", Xtr.shape, Xva.shape, Xte.shape)\n",
    "\n",
    "    models_q[h] = {}\n",
    "    for q in QUANTILES:\n",
    "        mdl = QuantileLinearGD(q=q, lr=Q_LR, epochs=Q_EPOCHS, batch_size=Q_BATCH, l2=Q_L2, seed=42, verbose=0)\n",
    "        mdl.fit(Xtr, ytr, Xva, yva)\n",
    "        models_q[h][q] = mdl\n",
    "\n",
    "    p10_v = models_q[h][0.1].predict(Xva)\n",
    "    p50_v = models_q[h][0.5].predict(Xva)\n",
    "    p90_v = models_q[h][0.9].predict(Xva)\n",
    "\n",
    "    p50_t = models_q[h][0.5].predict(Xte)\n",
    "\n",
    "    yva_real = np.expm1(yva)\n",
    "    p10_v_real = np.expm1(p10_v)\n",
    "    p90_v_real = np.expm1(p90_v)\n",
    "    \n",
    "    coverage_v = float(np.mean((yva_real >= p10_v_real) & (yva_real <= p90_v_real)))\n",
    "    width_v = float(np.mean(p90_v_real - p10_v_real))\n",
    "\n",
    "    rows.append({\n",
    "        \"horizon_days\": h,\n",
    "        \"val_rmse_p50\": rmse(yva, p50_v),\n",
    "        \"val_rmspe_p50\": rmspe(yva, p50_v), \n",
    "        \"val_mape_p50\": mape(yva, p50_v),\n",
    "        \"val_r2_p50\": r2(yva, p50_v),\n",
    "        \"val_pinball_p10\": pinball_loss(yva, p10_v, 0.1),\n",
    "        \"val_pinball_p50\": pinball_loss(yva, p50_v, 0.5),\n",
    "        \"val_pinball_p90\": pinball_loss(yva, p90_v, 0.9),\n",
    "        \"val_p10p90_coverage\": coverage_v,\n",
    "        \"val_p10p90_width\": width_v,\n",
    "        \"test_rmse_p50\": rmse(yte, p50_t),\n",
    "        \"test_rmspe_p50\": rmspe(yte, p50_t), \n",
    "        \"test_mape_p50\": mape(yte, p50_t),\n",
    "        \"test_r2_p50\": r2(yte, p50_t),\n",
    "    })\n",
    "\n",
    "metrics_q = pd.DataFrame(rows).sort_values(\"horizon_days\")\n",
    "save_df_csv(os.path.join(OUT_DIR, \"metrics_multistep_quantile.csv\"), metrics_q)\n",
    "display(metrics_q.head(7))\n",
    "\n",
    "# --- Plots ---\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(metrics_q[\"horizon_days\"], metrics_q[\"val_rmspe_p50\"], marker=\"o\", label=\"VAL RMSPE (p50)\")\n",
    "plt.plot(metrics_q[\"horizon_days\"], metrics_q[\"test_rmspe_p50\"], marker=\"o\", label=\"TEST RMSPE (p50)\")\n",
    "plt.title(\"RMSPE vs Horizon (Direct multi-step, 7 days)\")\n",
    "plt.xlabel(\"Horizon (days ahead)\")\n",
    "plt.ylabel(\"RMSPE\")\n",
    "plt.legend()\n",
    "save_fig(os.path.join(OUT_DIR, \"rmspe_vs_horizon.png\"))\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(metrics_q[\"horizon_days\"], metrics_q[\"val_p10p90_coverage\"], marker=\"o\")\n",
    "plt.title(\"P10–P90 Interval Coverage vs Horizon (VAL)\")\n",
    "plt.xlabel(\"Horizon (days ahead)\")\n",
    "plt.ylabel(\"Coverage\")\n",
    "plt.ylim(0,1)\n",
    "save_fig(os.path.join(OUT_DIR, \"coverage_vs_horizon.png\"))\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(metrics_q[\"horizon_days\"], metrics_q[\"val_p10p90_width\"], marker=\"o\")\n",
    "plt.title(\"Mean Interval Width (P90–P10) vs Horizon (VAL)\")\n",
    "plt.xlabel(\"Horizon (days ahead)\")\n",
    "plt.ylabel(\"Width (Euros)\")\n",
    "save_fig(os.path.join(OUT_DIR, \"interval_width_vs_horizon.png\"))\n",
    "\n",
    "# -------------------------\n",
    "# H) Forecast table (next 7 days) for sample stores + save CSV\n",
    "# -------------------------\n",
    "stores_unique = np.array(sorted(df_all[\"Store\"].unique()))\n",
    "rng = np.random.default_rng(42)\n",
    "sample_stores = rng.choice(stores_unique, size=min(SAMPLE_STORES_N, len(stores_unique)), replace=False)\n",
    "\n",
    "if FORECAST_ORIGIN_MODE == \"before_test\":\n",
    "    origin_date = test_start - pd.Timedelta(days=1)\n",
    "elif FORECAST_ORIGIN_MODE == \"custom\":\n",
    "    origin_date = pd.to_datetime(CUSTOM_ORIGIN_DATE)\n",
    "else:\n",
    "    origin_date = test_start - pd.Timedelta(days=1)\n",
    "\n",
    "base = df_all[df_all[\"Date\"] == origin_date].copy()\n",
    "base = base[base[\"Store\"].isin(sample_stores)].copy()\n",
    "base = base.dropna(subset=feature_cols)\n",
    "\n",
    "X0 = base[feature_cols].to_numpy(dtype=np.float32)\n",
    "stores0 = base[\"Store\"].to_numpy(dtype=int)\n",
    "\n",
    "forecast_rows = []\n",
    "for h in HORIZONS:\n",
    "    fdate = origin_date + pd.Timedelta(days=h)\n",
    "    p10 = models_q[h][0.1].predict(X0)\n",
    "    p50 = models_q[h][0.5].predict(X0)\n",
    "    p90 = models_q[h][0.9].predict(X0)\n",
    "    \n",
    "    p10_real = np.expm1(p10)\n",
    "    p50_real = np.expm1(p50)\n",
    "    p90_real = np.expm1(p90)\n",
    "    \n",
    "    for i in range(len(stores0)):\n",
    "        forecast_rows.append({\n",
    "            \"Store\": int(stores0[i]),\n",
    "            \"OriginDate\": origin_date.date().isoformat(),\n",
    "            \"ForecastDate\": fdate.date().isoformat(),\n",
    "            \"h\": int(h),\n",
    "            \"p10\": float(p10_real[i]),\n",
    "            \"p50\": float(p50_real[i]),\n",
    "            \"p90\": float(p90_real[i]),\n",
    "        })\n",
    "\n",
    "forecast_tbl = pd.DataFrame(forecast_rows)\n",
    "save_df_csv(os.path.join(OUT_DIR, \"forecast_sample_next_horizons.csv\"), forecast_tbl)\n",
    "display(forecast_tbl.head(7))\n",
    "\n",
    "# -------------------------\n",
    "# I) Ensemble uncertainty (optional) - on day 7\n",
    "# -------------------------\n",
    "def train_ensemble_median(h, K=7):\n",
    "    m_tr, m_va, _ = split_masks_for_h(df_all, h)\n",
    "    Xtr, ytr, _ = build_xy_for_h(df_all, h, m_tr)\n",
    "    Xva, yva, _ = build_xy_for_h(df_all, h, m_va)\n",
    "\n",
    "    ens = []\n",
    "    for k in range(K):\n",
    "        mdl = QuantileLinearGD(q=0.5, lr=Q_LR, epochs=max(10, Q_EPOCHS-3), batch_size=Q_BATCH, l2=Q_L2, seed=100+k, verbose=0)\n",
    "        mdl.fit(Xtr, ytr, Xva, yva)\n",
    "        ens.append(mdl)\n",
    "\n",
    "    preds_log = np.stack([m.predict(Xva) for m in ens], axis=0)\n",
    "    preds_real = np.expm1(preds_log)\n",
    "    \n",
    "    mu_real = preds_real.mean(axis=0)\n",
    "    sd_real = preds_real.std(axis=0)\n",
    "    yva_real = np.expm1(yva)\n",
    "\n",
    "    z = 1.6449  # ~90% interval\n",
    "    lo = mu_real - z*sd_real\n",
    "    hi = mu_real + z*sd_real\n",
    "    coverage = float(np.mean((yva_real >= lo) & (yva_real <= hi)))\n",
    "    \n",
    "    return (mu_real, sd_real, lo, hi, yva_real, coverage, float(np.sqrt(np.mean((yva_real - mu_real) ** 2))))\n",
    "\n",
    "if DO_ENSEMBLE and (ENSEMBLE_HORIZON in HORIZONS):\n",
    "    mu, sd, lo, hi, yva_e, cov_e, rmse_e = train_ensemble_median(ENSEMBLE_HORIZON, K=ENSEMBLE_K)\n",
    "    ens_metrics = pd.DataFrame([{\n",
    "        \"horizon_days\": ENSEMBLE_HORIZON,\n",
    "        \"K\": ENSEMBLE_K,\n",
    "        \"val_rmse_mean\": rmse_e,\n",
    "        \"val_coverage_approx90\": cov_e,\n",
    "        \"val_mean_std\": float(np.mean(sd)),\n",
    "        \"val_mean_interval_width\": float(np.mean(hi-lo)),\n",
    "    }])\n",
    "    save_df_csv(os.path.join(OUT_DIR, \"metrics_ensemble_uncertainty.csv\"), ens_metrics)\n",
    "    display(ens_metrics)\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(sd, bins=30)\n",
    "    plt.title(f\"Ensemble predictive std distribution (VAL) — h={ENSEMBLE_HORIZON}\")\n",
    "    plt.xlabel(\"std (Euros)\")\n",
    "    plt.ylabel(\"count\")\n",
    "    save_fig(os.path.join(OUT_DIR, f\"ensemble_std_hist_h{ENSEMBLE_HORIZON}.png\"))\n",
    "\n",
    "# -------------------------\n",
    "# J) Classification (low/med/high) + ROC-AUC + F1 (OvR)\n",
    "# -------------------------\n",
    "def make_class_labels(y_real, t1, t2):\n",
    "    y_real = np.asarray(y_real, dtype=np.float64)\n",
    "    cls = np.zeros_like(y_real, dtype=np.int64)\n",
    "    cls[(y_real > t1) & (y_real <= t2)] = 1\n",
    "    cls[y_real > t2] = 2\n",
    "    return cls\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "class SoftmaxRegressionGD:\n",
    "    def __init__(self, n_classes=3, lr=0.2, epochs=22, batch_size=8192, l2=1e-4, seed=42, verbose=1):\n",
    "        self.n_classes=n_classes; self.lr=lr; self.epochs=epochs; self.batch_size=batch_size\n",
    "        self.l2=l2; self.seed=seed; self.verbose=verbose\n",
    "        self.W=None; self.b=None\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        n, d = X.shape\n",
    "        k = self.n_classes\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.W = rng.normal(0, 0.01, size=(d, k)).astype(np.float32)\n",
    "        self.b = np.zeros((k,), dtype=np.float32)\n",
    "\n",
    "        best = (float(\"inf\"), self.W.copy(), self.b.copy())\n",
    "\n",
    "        def one_hot(y, k):\n",
    "            oh = np.zeros((len(y), k), dtype=np.float32)\n",
    "            oh[np.arange(len(y)), y] = 1.0\n",
    "            return oh\n",
    "\n",
    "        for ep in range(1, self.epochs+1):\n",
    "            idx = rng.permutation(n)\n",
    "            Xs, ys = X[idx], y[idx]\n",
    "\n",
    "            for i in range(0, n, self.batch_size):\n",
    "                xb = Xs[i:i+self.batch_size]\n",
    "                yb = ys[i:i+self.batch_size]\n",
    "                Y = one_hot(yb, k)\n",
    "\n",
    "                logits = xb @ self.W + self.b\n",
    "                P = softmax(logits)\n",
    "\n",
    "                G = (P - Y) / len(xb)\n",
    "                gW = xb.T @ G\n",
    "                gb = np.sum(G, axis=0)\n",
    "\n",
    "                if self.l2 > 0:\n",
    "                    gW = gW + self.l2 * self.W\n",
    "\n",
    "                self.W -= self.lr * gW\n",
    "                self.b -= self.lr * gb\n",
    "\n",
    "            if X_val is not None:\n",
    "                Pv = self.predict_proba(X_val)\n",
    "                ce = -np.mean(np.log(Pv[np.arange(len(y_val)), y_val] + 1e-12))\n",
    "                if ce < best[0]:\n",
    "                    best = (ce, self.W.copy(), self.b.copy())\n",
    "                if self.verbose:\n",
    "                    print(f\"Epoch {ep:02d} | Val CE={ce:.4f}\")\n",
    "\n",
    "        if X_val is not None:\n",
    "            _, self.W, self.b = best\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return softmax(X @ self.W + self.b).astype(np.float32)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "def f1_macro(y_true, y_pred, n_classes=3):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.int64)\n",
    "    f1s = []\n",
    "    for c in range(n_classes):\n",
    "        tp = np.sum((y_true==c) & (y_pred==c))\n",
    "        fp = np.sum((y_true!=c) & (y_pred==c))\n",
    "        fn = np.sum((y_true==c) & (y_pred!=c))\n",
    "        prec = tp / (tp + fp + 1e-12)\n",
    "        rec  = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2*prec*rec / (prec + rec + 1e-12)\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "def binary_auc_roc(y_true_bin, y_score):\n",
    "    y_true_bin = np.asarray(y_true_bin, dtype=np.int64)\n",
    "    y_score = np.asarray(y_score, dtype=np.float64)\n",
    "    pos = (y_true_bin == 1)\n",
    "    neg = (y_true_bin == 0)\n",
    "    n_pos = int(np.sum(pos)); n_neg = int(np.sum(neg))\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return np.nan\n",
    "\n",
    "    ranks = pd.Series(y_score).rank(method=\"average\").to_numpy(dtype=np.float64)\n",
    "    sum_ranks_pos = float(np.sum(ranks[pos]))\n",
    "    auc = (sum_ranks_pos - n_pos*(n_pos+1)/2.0) / (n_pos*n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "def auc_roc_ovr_macro(y_true, proba, n_classes=3):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    proba = np.asarray(proba, dtype=np.float64)\n",
    "    aucs = []\n",
    "    for c in range(n_classes):\n",
    "        y_bin = (y_true == c).astype(int)\n",
    "        auc = binary_auc_roc(y_bin, proba[:, c])\n",
    "        if not np.isnan(auc):\n",
    "            aucs.append(auc)\n",
    "    return float(np.mean(aucs)) if len(aucs) else np.nan\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, n_classes=3):\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for t,p in zip(y_true, y_pred):\n",
    "        cm[int(t), int(p)] += 1\n",
    "    return cm\n",
    "\n",
    "clf_rows = []\n",
    "\n",
    "for h in CLASS_HORIZONS:\n",
    "    if h > H_FULL:\n",
    "        continue\n",
    "\n",
    "    m_tr, m_va, m_te = split_masks_for_h(df_all, h)\n",
    "    Xtr, ytr, _ = build_xy_for_h(df_all, h, m_tr)\n",
    "    Xva, yva, _ = build_xy_for_h(df_all, h, m_va)\n",
    "    Xte, yte, _ = build_xy_for_h(df_all, h, m_te)\n",
    "\n",
    "    ytr_real = np.expm1(ytr)\n",
    "    yva_real = np.expm1(yva)\n",
    "    yte_real = np.expm1(yte)\n",
    "\n",
    "    t1 = np.quantile(ytr_real, 1/3)\n",
    "    t2 = np.quantile(ytr_real, 2/3)\n",
    "\n",
    "    ytr_c = make_class_labels(ytr_real, t1, t2)\n",
    "    yva_c = make_class_labels(yva_real, t1, t2)\n",
    "    yte_c = make_class_labels(yte_real, t1, t2)\n",
    "\n",
    "    print(f\"\\n=== Classification horizon h={h} ===\")\n",
    "    print(f\"Thresholds (Euros): Low < {t1:.1f} <= Med <= {t2:.1f} < High\")\n",
    "    print(\"Train class counts:\", np.bincount(ytr_c, minlength=3))\n",
    "\n",
    "    clf = SoftmaxRegressionGD(n_classes=3, lr=CLF_LR, epochs=CLF_EPOCHS, batch_size=CLF_BATCH, l2=CLF_L2, verbose=0)\n",
    "    clf.fit(Xtr, ytr_c, Xva, yva_c)\n",
    "\n",
    "    Pva = clf.predict_proba(Xva)\n",
    "    Pte = clf.predict_proba(Xte)\n",
    "    yva_pred = np.argmax(Pva, axis=1)\n",
    "    yte_pred = np.argmax(Pte, axis=1)\n",
    "\n",
    "    val_f1 = f1_macro(yva_c, yva_pred, 3)\n",
    "    val_auc = auc_roc_ovr_macro(yva_c, Pva, 3)\n",
    "    test_f1 = f1_macro(yte_c, yte_pred, 3)\n",
    "    test_auc = auc_roc_ovr_macro(yte_c, Pte, 3)\n",
    "\n",
    "    clf_rows.append({\n",
    "        \"horizon_days\": h,\n",
    "        \"t1\": float(t1),\n",
    "        \"t2\": float(t2),\n",
    "        \"val_f1_macro\": val_f1,\n",
    "        \"val_auc_ovr_macro\": val_auc,\n",
    "        \"test_f1_macro\": test_f1,\n",
    "        \"test_auc_ovr_macro\": test_auc,\n",
    "    })\n",
    "\n",
    "    cm_val = confusion_matrix(yva_c, yva_pred, 3)\n",
    "    cm_test = confusion_matrix(yte_c, yte_pred, 3)\n",
    "\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.imshow(cm_val, aspect=\"auto\")\n",
    "    plt.title(f\"Confusion Matrix (VAL) — h={h}\")\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "    plt.colorbar()\n",
    "    save_fig(os.path.join(OUT_DIR, f\"cm_val_h{h}.png\"))\n",
    "\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.imshow(cm_test, aspect=\"auto\")\n",
    "    plt.title(f\"Confusion Matrix (TEST) — h={h}\")\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "    plt.colorbar()\n",
    "    save_fig(os.path.join(OUT_DIR, f\"cm_test_h{h}.png\"))\n",
    "\n",
    "clf_metrics = pd.DataFrame(clf_rows).sort_values(\"horizon_days\")\n",
    "save_df_csv(os.path.join(OUT_DIR, \"metrics_classification.csv\"), clf_metrics)\n",
    "display(clf_metrics)\n",
    "\n",
    "# -------------------------\n",
    "# K) Quick summary text file\n",
    "# -------------------------\n",
    "summary_lines = []\n",
    "summary_lines.append(\"Phase 4 outputs saved in artifacts/phase4/\")\n",
    "summary_lines.append(f\"Horizons trained: {HORIZONS}\")\n",
    "summary_lines.append(\"Files:\")\n",
    "summary_lines.append(\"- metrics_multistep_quantile.csv\")\n",
    "summary_lines.append(\"- rmspe_vs_horizon.png\")\n",
    "summary_lines.append(\"- coverage_vs_horizon.png\")\n",
    "summary_lines.append(\"- interval_width_vs_horizon.png\")\n",
    "summary_lines.append(\"- forecast_sample_next_horizons.csv\")\n",
    "summary_lines.append(\"- metrics_classification.csv\")\n",
    "if DO_ENSEMBLE and (ENSEMBLE_HORIZON in HORIZONS):\n",
    "    summary_lines.append(\"- metrics_ensemble_uncertainty.csv\")\n",
    "    summary_lines.append(f\"- ensemble_std_hist_h{ENSEMBLE_HORIZON}.png\")\n",
    "for h in CLASS_HORIZONS:\n",
    "    summary_lines.append(f\"- cm_val_h{h}.png / cm_test_h{h}.png\")\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"SUMMARY.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(summary_lines))\n",
    "\n",
    "print(\"\\n\".join(summary_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c796c8c-510f-402f-8c2b-9a5f2d6c774d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
